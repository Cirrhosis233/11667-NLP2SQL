{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompt_to_dataset(dataset, prompt_file=\"prompt.md\"):\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    def prepare_prompt(batch):\n",
    "        batch[\"prompt\"] = [\n",
    "            prompt.format(user_question=q, table_metadata_string=m)\n",
    "            for q, m in zip(batch[\"question\"], batch[\"context\"])\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    dataset = dataset.map(prepare_prompt, batched=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16, # May not work on All GPU, use float16 instead if error\n",
    "        device_map=\"auto\",\n",
    "        use_cache=True,\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(input, tokenizer, model):\n",
    "    prompt = generate_prompt(input)\n",
    "    \n",
    "    # make sure the model stops generating at triple ticks\n",
    "    # eos_token_id = tokenizer.convert_tokens_to_ids([\"```\"])[0]\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False,\n",
    "        return_full_text=False, # added return_full_text parameter to prevent splitting issues with prompt\n",
    "        num_beams=1, # do beam search with 4 beams for high quality results\n",
    "    )\n",
    "    generated_query = (\n",
    "        pipe(\n",
    "            prompt,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=eos_token_id,\n",
    "            pad_token_id=eos_token_id,\n",
    "        )[0][\"generated_text\"]\n",
    "        .split(\";\")[0]\n",
    "        .split(\"```\")[0]\n",
    "        .strip()\n",
    "    )\n",
    "    return generated_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, original_list):\n",
    "        self.original_list = original_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.original_list[i]\n",
    "\n",
    "\n",
    "def generate_sql_batch(dataset, tokenizer, model, batch_size=16):\n",
    "    dataset = add_prompt_to_dataset(dataset)\n",
    "    dataset = ListDataset(dataset)\n",
    "\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token_id = eos_token_id\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "        num_beams=1,\n",
    "    )\n",
    "\n",
    "    all_generations = []\n",
    "\n",
    "    for output in tqdm(\n",
    "        pipe(\n",
    "            dataset[\"prompt\"],\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=eos_token_id,\n",
    "            pad_token_id=eos_token_id,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    ):\n",
    "        generated_query = (\n",
    "            output[0][\"generated_text\"].split(\";\")[0].split(\"```\")[0].strip()\n",
    "        )\n",
    "        all_generations.append(generated_query)\n",
    "\n",
    "    return all_generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a4f4e4b1e8400caa414a8f7275a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"./datasets/sql-create-context-split\")\n",
    "tokenizer, model = get_tokenizer_model(\"defog/sqlcoder-7b-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"train\"].take(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### value for proper max token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_lengths = [\n",
    "#     len(tokenizer.tokenize(answer)) for answer in dataset[\"train\"][\"answer\"]\n",
    "# ]\n",
    "\n",
    "# # Analyze statistics\n",
    "# max_length = max(token_lengths)\n",
    "# mean_length = sum(token_lengths) / len(token_lengths)\n",
    "# print(f\"Max tokens: {max_length}, Mean tokens: {mean_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 808540.53it/s]\n"
     ]
    }
   ],
   "source": [
    "generations = generate_sql_batch(test_dataset, tokenizer, model, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.add_column(\"generation\", generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'SELECT COUNT(clock_rate__mhz_) FROM table_142573_1 WHERE bandwidth__mb_s_ = 2400',\n",
       " 'question': 'Name the number of clock rate mhz when bandwidth mb/s is 2400',\n",
       " 'context': 'CREATE TABLE table_142573_1 (clock_rate__mhz_ VARCHAR, bandwidth__mb_s_ VARCHAR)',\n",
       " 'generation': \"SELECT t.clock_rate__mhz_ FROM table_142573_1 t WHERE t.bandwidth__mb_s_ = '2400'\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_batch(batch, prompt_file=\"prompt.md\"):\n",
    "    with open(prompt_file, \"r\") as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    prompt_batch = [\n",
    "        prompt.format(user_question=q, table_metadata_string=m)\n",
    "        for q, m in zip(batch[\"question\"], batch[\"context\"])\n",
    "    ]\n",
    "\n",
    "    return prompt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_sql_batch(dataset, tokenizer, model, batch_size=1):\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token_id = eos_token_id\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    generations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Generating:\"):\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        prompts = generate_prompt_batch(batch)\n",
    "\n",
    "        outputs = pipe(\n",
    "            prompts,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=eos_token_id,\n",
    "            pad_token_id=eos_token_id,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        for output in outputs:\n",
    "            generated_query = (\n",
    "                output[0][\"generated_text\"].split(\";\")[0].split(\"```\")[0].strip()\n",
    "            )\n",
    "            generations.append(generated_query)\n",
    "\n",
    "    return generations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
