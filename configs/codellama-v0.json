{
    "model": "codellama/CodeLlama-7b-Instruct-hf",
    "dataset": "./datasets/sql-create-context-split",
    "prompt": "./prompts/prompt_v2.md",
    "output_dir": "./models/codellama-v1",
    "report_to": "wandb",
    "run_name": "codellama-v1",
    "eval_strategy": "steps",
    "eval_steps": 5,
    "logging_steps": 1,
    "seq_len": 1024,
    "bits": 4,
    "bnb_4bit_quant_type": "nf4",
    "r": 128,
    "lora_alpha": 128,
    "lora_dropout": 0.1,
    "target_modules": "all-linear",
    "bias": "none",
    "init_lora_weights": true,
    "task_type": "CAUSAL_LM",
    "num_train_epochs": 1,
    "save_strategy": "no",
    "bf16": true,
    "fp16": false,
    "gradient_checkpointing": true,
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 4,
    "lr_scheduler_type": "cosine",
    "learning_rate": 5e-4,
    "weight_decay": 0.01,
    "warmup_ratio": 0.05,
    "gradient_accumulation_steps": 2
}